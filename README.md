# FIAP Tech Challenge

![Spring](https://img.shields.io/badge/spring-%236DB33F.svg?style=for-the-badge&logo=spring&logoColor=white)
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
![Maven](https://img.shields.io/badge/maven-%230db7ed.svg?style=for-the-badge&logo=maven&logoColor=white)
![Swagger](https://img.shields.io/badge/-Swagger-%23Clojure?style=for-the-badge&logo=swagger&logoColor=white)
![RabbitMQ](https://img.shields.io/badge/Rabbitmq-FF6600?style=for-the-badge&logo=rabbitmq&logoColor=white)

[![CI Status](https://github.com/ra1nmak3r1/tech_challenge_fiap/actions/workflows/docker-build.yml/badge.svg)](https://github.com/ra1nmak3r1/tech_challenge_fiap/actions)
[![SonarQube](https://github.com/ns-fiap-tc/tech_challenge_fiap/actions/workflows/sonarcloud.yml/badge.svg)](https://github.com/ns-fiap-tc/tech_challenge_fiap/actions/workflows/sonarcloud.yml)
[![Terraform Status](https://github.com/ns-fiap-tc/tech_challenge_fiap/actions/workflows/deploy.yml/badge.svg)](https://github.com/ns-fiap-tc/tech_challenge_fiap/actions)

## üìñ Sobre o projeto

Tech Challenge do curso Software Architecture da FIAP.

### 1Ô∏è‚É£ Fase 1

> Aplica√ß√£o desenvolvida utilizando arquitetura hexagonal que contempla a gest√£o dos pedidos de uma lanchonete.
>
> O c√≥digo fonte inalterado desta fase ainda pode ser encontrado na branch [`release/v1.0.0`](https://github.com/ra1nmak3r1/tech_challenge_fiap/tree/release/v1.0.0)

### 2Ô∏è‚É£ Fase 2

> Migra√ß√£o da aplica√ß√£o da arquitetura hexagonal para clean architecture.

### üìù Sobre a refatora√ß√£o na aplicac√£o para a Fase 2

1. Por conta do refactoring para clean architecture, uma situa√ß√£o que enfrentamos foi a aus√™ncia do contexto transacional do Spring na utiliza√ß√£o das classes de neg√≥cios quando executavam o m√≥dulo de persist√™ncia (JPA), uma vez que as classes de neg√≥cios (\*UseCasesImpl) n√£o estavam mais sendo gerenciadas pelo ApplicationContext do Spring. Como solu√ß√£o para este cen√°rio, utilizamos AOP (Programa√ß√£o Orientada a Aspectos) para interceptar as chamadas aos m√©todos dos Controllers (que est√£o sendo gerenciados pelo Spring) para incluirmos cada execu√ß√£o em uma transa√ß√£o isolada.

2. Alteramos a estrutura do projeto em sub-m√≥dulos, sendo eles:

   - business: cont√©m as classes de neg√≥cios, que fazem parte do core da aplica√ß√£o e que podem ser executados com diferentes recursos externos, sendo utilizados os frameworks: lombok e mapstruct - ambos utilizados na gera√ß√£o de c√≥digo em tempo de compila√ß√£o.
   - app: cont√©m as classes relacionadas aos frameworks e recursos utilizados para o correto funcionamento da aplica√ß√£o.
   - pagamento-mock: aplica√ß√£o apartada que simula a execu√ß√£o do Mercado Pago para efetiva√ß√£o do pagamento do pedido.

3. No m√≥dulo business, foram utilizados apenas os frameworks lombok e mapstruct.

   - O lombok √© utilizado para a gera√ß√£o de m√©todos getters, setters, hashCode, equals e construtores.

   - Enquanto o mapstruct √© utilizado para a cria√ß√£o de m√©todos que fazem o mapeamento dos atributos entre entidades para realiza√ß√£o da c√≥pia dos valores dos atributos de beans de classes diferentes.

4. Utilizamos os presenters apenas como sendo a transforma√ß√£o dos beans de dom√≠nio pra os DTOs a serem enviados para fora dos Controllers. Nesta implementa√ß√£o os DTOs s√£o os mesmos utilizados no recebimento dos m√©todos externos e como informa√ß√£o a ser retornada, mas em caso de altera√ß√£o da informa√ß√£o retornada, basta alterar o tipo de retorno dos m√©todos dos Controllers e os presenters.

<details>
  <summary>Detalhamento estrutura e execu√ß√£o na Fase 2</summary>

## üèõÔ∏è Estrutura utilizada nos pacotes

```
ra√≠z
‚îú‚îÄ‚îÄ app (m√≥dulo)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src
‚îÇ       ‚îî‚îÄ‚îÄ application
‚îÇ           ‚îú‚îÄ‚îÄ	device
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ queue (produtores / consumidores)
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ rest (interfaces)
‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exception
‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handler
‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ impl (implementa√ß√µes das interfaces)
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ persistence
‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ entity
‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ mapper
‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ repository
‚îÇ           ‚îî‚îÄ‚îÄ infrastructure (local onde ser√£o utilizadas as depend√™ncias de cada cloud ou de recursos externos)
‚îÇ               ‚îú‚îÄ‚îÄ aspect (pacote contendo as classes da AOP)
‚îÇ               ‚îú‚îÄ‚îÄ config (inclus√£o das configura√ß√µes da aplica√ß√£o, como por exemplo @Configuration do Spring, criando os @Bean)
‚îÇ               ‚îú‚îÄ‚îÄ utils (classes utilit√°rias)
‚îÇ               ‚îî‚îÄ‚îÄ aws (pacotes espec√≠ficos para cada cloud, por exemplo)
‚îú‚îÄ‚îÄ business (m√≥dulo)
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src
‚îÇ       ‚îî‚îÄ‚îÄ business (classes / interfaces referentes √†s regras de neg√≥cios da aplica√ß√£o. criar as classes / interfaces sem usar frameworks - c√≥digo o mais simples poss√≠vel)
‚îÇ           ‚îú‚îÄ‚îÄ adapter
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ controller
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ gateway
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ presenter
‚îÇ           ‚îú‚îÄ‚îÄ common
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ dto
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ mapper
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ queue (produtor / consumidor)
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ persistence
‚îÇ           ‚îî‚îÄ‚îÄ core
‚îÇ               ‚îú‚îÄ‚îÄ domain (POJOs)
‚îÇ               ‚îú‚îÄ‚îÄ exception
‚îÇ               ‚îî‚îÄ‚îÄ usecase (interfaces contendo os m√©todos a serem implementados)
‚îÇ                   ‚îî‚îÄ‚îÄ impl (implementa√ß√µes das interfaces)
‚îî‚îÄ‚îÄ pagamento-mock (m√≥dulo)
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ pom.xml
    ‚îî‚îÄ‚îÄ src
        ‚îî‚îÄ‚îÄ pagamentomock
            ‚îú‚îÄ‚îÄ adapter
            ‚îÇ   ‚îú‚îÄ‚îÄ input
            ‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ controller
            ‚îÇ	‚îÇ    ‚îú‚îÄ‚îÄ dto
            ‚îÇ	‚îÇ    ‚îî‚îÄ‚îÄ mapper
            ‚îÇ	‚îî‚îÄ‚îÄ output
            ‚îî‚îÄ‚îÄ infrastructure
                ‚îú‚îÄ‚îÄ config
                ‚îî‚îÄ‚îÄ utils

```

## üíª Tecnologias utilizadas na Aplica√ß√£o

- Maven 3.9.9
- Spring Boot 3.3.4
- Java 17

## üì¶ Arquitetura da Infraestrutura e CI/CD

### üöÄ Tecnologias Utilizadas na Infraestrutura

- **Minikube** ‚Äî Cluster Kubernetes local para simular produ√ß√£o
- **Kubernetes (K8s)** ‚Äî Orquestra√ß√£o dos recursos
- **Docker** ‚Äî Empacotamento das aplica√ß√µes em containers
- **Docker Compose** ‚Äî Suporte ao ambiente de desenvolvimento local
- **GitHub Actions** ‚Äî Pipeline de CI para build e deploy das imagens
- **Docker Hub** ‚Äî Reposit√≥rio para armazenar imagens da aplica√ß√£o
- **Secrets e ConfigMaps** ‚Äî Gest√£o segura de vari√°veis sens√≠veis no cluster
- **RabbitMQ(4.0.5) & PostgreSQL(16)** ‚Äî Infraestrutura de mensageria e banco de dados

---

### ‚úÖ Pr√©-requisitos para Execu√ß√£o

- **Docker** e **Docker Compose** instalados
- **Minikube** instalado e configurado localmente (Testes e valida√ß√µes realizados com a v1.35.0)
- Acesso ao `.env` com as vari√°veis necess√°rias

---

### üõ†Ô∏è Integra√ß√£o Cont√≠nua (CI)

O fluxo de CI √© automatizado via **GitHub Actions** e √© engatilhado a cada `push` na branch `main`.

1. Faz o checkout do reposit√≥rio
2. Gera as imagens Docker de cada aplica√ß√£o (`app` e `mock-pagamento`)
3. Faz o login no Docker Hub usando um **Access Token seguro**
4. Publica as imagens no Docker Hub (`app` em reposit√≥rio privado e `mock-pagamento` em reposit√≥rio p√∫blico)

---

### üåê Deploy e Infraestrutura (CD)

A subida do ambiente √© feita localmente via script `setup.sh` ou `setup.bat`, que:

1. L√™ e carrega o arquivo `.env` com credenciais e configura√ß√µes
2. Cria dinamicamente as Secrets no Kubernetes
3. Aplica todos os manifestos do cluster (PostgreSQL, RabbitMQ, app e mock)
4. Exp√µe os servi√ßos via `port-forward` para acesso local (`localhost:8080`, `:8081`)

---

### üß≠ Fluxo da Arquitetura (CI/CD)

```mermaid
flowchart TD
    dev[Desenvolvedor] --> pushGit[Push para GitHub]

    subgraph GitHub_Actions_CI
        pushGit --> buildApp[Build imagem da aplica√ß√£o]
        buildApp --> buildMock[Build imagem do mock-pagamento]
        buildMock --> dockerLogin[Login no Docker Hub]
        dockerLogin --> pushImages[Push das imagens]
    end

    pushImages --> dockerHub[Docker Hub]

    dev --> runSetup[Executa setup.sh ou setup.bat]

    subgraph Execucao_Local_CD
        runSetup --> loadEnv[Carrega vari√°veis do .env]
        loadEnv --> createSecrets[Cria Secrets no Kubernetes]
        createSecrets --> applyManifests[Aplica manifestos do cluster]
        applyManifests --> portForward[Port-forward dos servi√ßos]
    end

    dockerHub --> applyManifests
    portForward --> appAccess[Acesso ao App Principal na porta 8080]
    portForward --> mockAccess[Acesso ao Mock Pagamento na porta 8081]
```

### üìà Escalabilidade e HPA no Kubernetes

Para lidar com cen√°rios de alta demanda, como por exemplo **lentid√£o no totem da lanchonete durante hor√°rios de pico**, a aplica√ß√£o principal foi configurada com um recurso chamado **HPA (Horizontal Pod Autoscaler)** no Kubernetes.

O HPA monitora o uso de **CPU** do container da aplica√ß√£o e **escala automaticamente o n√∫mero de r√©plicas** (pods) quando a utiliza√ß√£o ultrapassa um determinado limite configurado.

---

#### üìå Exemplo de caso pr√°tico

> Cen√°rio: durante o hor√°rio de almo√ßo, h√° um grande volume de clientes utilizando o totem de autoatendimento. Isso gera lentid√£o e demora nas respostas da aplica√ß√£o.

üîß Solu√ß√£o:

- O HPA detecta que o uso de CPU no pod principal (`app`) ou no `mock-pagamento` est√° acima do limite (70% para o app, 80% para o mock)
- Ele automaticamente cria novos pods (`r√©plicas`) da aplica√ß√£o para distribuir a carga
- Os **Services do Kubernetes** atuam como balanceadores de carga, redirecionando requisi√ß√µes para os pods dispon√≠veis
- Quando o pico passa, o HPA reduz o n√∫mero de pods novamente para economizar recursos

---

#### üß≠ Diagrama da Escalabilidade

```mermaid
flowchart TD
    Client[Usuario - Totem/Navegador] --> Ingress[Entrada de Requisi√ß√µes]
    Ingress --> AppService[Service - app]
    Ingress --> MockService[Service - mock-pagamento]

    subgraph App_Pods
        App1[Pod app - R√©plica 1]
        App2[Pod app - R√©plica 2]
        AppN[Pod app - R√©plica N]
    end

    subgraph Mock_Pods
        Mock1[Pod mock - R√©plica 1]
        Mock2[Pod mock - R√©plica 2]
        MockN[Pod mock - R√©plica N]
    end

    AppService --> App1
    AppService --> App2
    AppService --> AppN

    MockService --> Mock1
    MockService --> Mock2
    MockService --> MockN

    subgraph HPA[Horizontal Pod Autoscaler]
        HPAApp[Escala o app baseado em CPU >70%]
        HPAMock[Escala o mock baseado em CPU >80%]
    end

    HPAApp --> App1
    HPAApp --> App2
    HPAMock --> Mock1
    HPAMock --> Mock2
```

---

#### üßë‚Äçüíª Considera√ß√µes

- O **HPA est√° configurado para ambos os servi√ßos**:
  - `app` com threshold de 70% de uso de CPU
  - `mock-pagamento` com threshold de 80% de uso de CPU
- O `app` pode escalar at√© **5 r√©plicas**, conforme demanda
- O `mock-pagamento` pode escalar at√© **3 r√©plicas**, conforme demanda
  > No `mock-pagamento` estamos apenas simulando um sistema externo de pagamentos, n√£o necessariamente precisar√≠amos de um HPA nele, mas decidimos manter a configura√ß√£o em uma escala menor

## ‚öôÔ∏è Como executar a infraestrutura com Minikube

### ‚úÖ 1. Pr√©-requisitos

Instale as ferramentas abaixo:

- [Docker](https://www.docker.com/products/docker-desktop/)
- [Minikube](https://minikube.sigs.k8s.io/docs/start/)

---

### ‚úÖ 2. Clonar o reposit√≥rio

```bash
git clone https://github.com/ra1nmak3r1/tech_challenge_fiap.git
cd tech_challenge_fiap
```

---

### ‚úÖ 3. Criar o arquivo `.env` com base no `.env.example`

J√° existe um arquivo de exemplo chamado **`.env.example`** no projeto.

<details>
  <summary><strong>üîê COMO CONFIGURAR O ARQUIVO .ENV</strong></summary>

1. Copie o arquivo `.env.example` para `.env`:

```bash
cp .env.example .env  # Linux ou Mac
```

```powershell
copy .env.example .env  # Windows
```

2. Substitua os valores fict√≠cios pelos **valores reais que foram enviados separadamente**.

> ‚ö†Ô∏è Os valores do `.env.example` s√£o apenas ilustrativos e n√£o funcionais.

</details>

---

### ‚úÖ 4. Subir a infraestrutura

Foram desenvolvidos scripts em `.sh` e `.bat` para facilitar a inicializa√ß√£o da infraestrutura no Minikube, sem que seja necess√°rio executar os comandos da API do Kubernetes para tal. Na pr√°tica ambos os scripts fazem o seguinte:

1. ‚úÖ Verifica se o arquivo `.env` existe e carrega suas vari√°veis
2. üîê Cria dinamicamente as **Secrets** no Kubernetes com base no `.env`
3. üê≥ Cria uma Secret para autentica√ß√£o no **Docker Hub** (para acesso √† imagem privada da aplica√ß√£o principal)
4. üöÄ Inicializa o cluster local do **Minikube**
5. üìÇ Aplica todos os **manifestos Kubernetes** da aplica√ß√£o:
   - Banco de dados PostgreSQL
   - RabbitMQ
   - Aplica√ß√£o principal (`app`)
   - Servi√ßo de mock de pagamento (`mock-pagamento`)
6. üåê Exp√µe os servi√ßos localmente via `kubectl port-forward`, permitindo acesso via `localhost`
7. ‚è≥ Aguarda os pods ficarem prontos antes de liberar o acesso

> ‚ÑπÔ∏è Em cerca de 3-4 minutos, o ambiente estar√° funcionando localmente com todos os microsservi√ßos no ar.

#### ‚ñ∂Ô∏è Linux ou Mac:

```bash
chmod +x setup.sh
./setup.sh
```

#### ü™ü Windows:

```powershell
.\setup.bat
```

---

### ‚úÖ 5. Acessar os servi√ßos localmente

| Servi√ßo        | URL                   |
| -------------- | --------------------- |
| Aplica√ß√£o      | http://localhost:8080 |
| Mock Pagamento | http://localhost:8081 |

---

### üõ†Ô∏è Comandos √∫teis para observa√ß√£o

Ver todos os pods:

```bash
kubectl get pods
```

Ver logs da aplica√ß√£o principal:

```bash
kubectl logs -l app=lanchonete-app -f
```

Ver logs do mock pagamento:

```bash
kubectl logs -l app=mock-pagamento -f
```

---

### üßπ Resetar tudo (opcional)

Caso queira limpar o ambiente e recome√ßar do zero, preparamos os seguintes execut√°veis para facilitar o processo:

#### ‚ñ∂Ô∏è Linux ou Mac

```bash
./delete_setup.sh
```

#### ü™ü Windows

```powershell
.\delete_setup.bat
```

Ou, voc√™ tamb√©m pode zerar o minikube por completo se desejar

```bash
minikube delete
```

## üìÑ Acesso √† documenta√ß√£o das APIs

#### Aplica√ß√£o

- http://localhost:8080/api-docs (endpoints)
- http://localhost:8080/swagger-ui/index.html (swagger-ui)

#### Pagamento Mock

- http://localhost:8081/api-docs (endpoints)
- http://localhost:8081/swagger-ui/index.html (swagger-ui)

## üß™ Execu√ß√£o em modo de Desenvolvimento (sem Minikube)

<details>

<summary>Se desejar executar a aplica√ß√£o em modo de desenvolvimento local para debugar e alterar o c√≥digo fonte em car√°ter de teste, siga este passo a passo</summary>

#### ‚úÖ 1. Pr√©-requisitos

- Docker
- Docker Compose
- Maven 3.9.9
- Spring Boot 3.3.4
- Java 17

---

#### ‚úÖ 2. Gerar o `.env`

Crie o arquivo `.env` com base no `.env.example`, da mesma forma descrita anteriormente:

```bash
cp .env.example .env  # Linux ou Mac
```

```powershell
copy .env.example .env  # Windows
```

Substitua os valores conforme os dados enviados.

---

#### ‚úÖ 3. Buildar as aplica√ß√µes localmente (apenas na primeira vez)

Primeiro instale o pacote parent da aplica√ß√£o, atrav√©s do comando:

```bash
mvn -DskipTests -DskipITs=true -N clean install
```

Em seguida, compile o projeto e gere o arquivo `.jar`. Para isso, execute:

```bash
mvn -DskipTests clean package
```

---

#### ‚úÖ 4. Subir o ambiente de desenvolvimento com Docker Compose

Na raiz do projeto, execute:

```bash
docker compose up --build
```

Isso ir√°:

- Buildar os containers da aplica√ß√£o principal e do mock
- Subir o banco de dados PostgreSQL e o RabbitMQ
- Conectar todos os servi√ßos em rede local

---

#### ‚úÖ 5. Acessar os servi√ßos localmente

| Servi√ßo        | URL                   |
| -------------- | --------------------- |
| Aplica√ß√£o      | http://localhost:8080 |
| Mock Pagamento | http://localhost:8081 |

</details>

## üîÑ Fluxo de Execu√ß√£o

### 1. Cria√ß√£o de um novo pedido

```
POST -> /pedido-service/v1/save
```

---

### 2. Atualiza√ß√£o dos itens do pedido

```
PUT -> /pedido-service/v1/save/:id
```

Esta atualiza√ß√£o contempla:

- Os **itens do pedido**
- As **informa√ß√µes de pagamento**
- O **status do pedido**

> O ID do objeto pagamento deve ser preenchido.

Caso o status seja alterado para `RECEBIDO`, isso significa que o pedido foi finalizado pelo usu√°rio e agora ser√° feito o **processamento do pagamento**, que ocorrer√° de forma ass√≠ncrona, utilizando o projeto **Pagamento Mock**.

---

### 3. Confirma√ß√£o do Pagamento

Para confirmar que o pagamento foi realizado, √© necess√°rio executar o endpoint abaixo do **Pagamento Mock**, que por sua vez **executar√° o webhook da aplica√ß√£o**.

**Endpoint do Pagamento Mock:**

```
POST -> /pagamento-mock-service/v1/callPagamentoWebHook/:pedidoId/:aprovarPagamento
```

**Webhook da aplica√ß√£o:**

```
POST -> /pagamento-service/v1/updateStatus/:pedidoId/:statusCode
```

### Observa√ß√µes

- Ambos os m√©todos foram definidos como `POST` por n√£o serem indepotentes.
- A execu√ß√£o do webhook, caso receba o `statusCode = 100`, significa que o pagamento foi realizado com sucesso, e far√° com que o pedido seja **confirmado** e as **Ordens de Servi√ßo sejam criadas para a cozinha**.

</details>

### 3Ô∏è‚É£ Fase 3

> Migra√ß√£o da aplica√ß√£o para AWS, automatizando a cria√ß√£o da infra-estrutura com o terraform.

1. O banco de dados foi migrado para a AWS RDS utilizando o engine do PostgreSQL, que era o banco que j√° era usado pela aplica√ß√£o. N√£o foram realizadas altera√ß√µes na estrutura da base de dados, por j√° existirem todos os campos necess√°rios.

2. Foi inclu√≠da a utiliza√ß√£o do servi√ßo AWS Lambda para consultar a exist√™ncia do CPF do cliente na base de dados, caso seja informado inicialmente. Sendo inclu√≠do o CPF no JWT que ser√° criado neste momento. Caso n√£o exista ou n√£o seja informado, o JWT ser√° criado com CPF vazio.

3. Dentro da pasta `terraform` cont√©m c√≥digos Terraform para provisionar os deployments necess√°rios para rodar a aplica√ß√£o.

<details>
  <summary>Detalhamento estrutura e execu√ß√£o na Fase 3</summary>

## üîÅ Fluxo da requisi√ß√£o na AWS

![Fluxo AWS](fluxo-aws.png "T√≠tulo Opcional da Imagem")

## üëü Passos para o provisionamento
Este projeto faz parte de um ecossistema maior, composto por m√∫ltiplos reposit√≥rios que se comunicam entre si e tamb√©m utilizam GitHub Actions para provisionamento ou deploy automatizado.

Para completo funcionamento da plataforma, √© necess√°rio seguir o seguinte fluxo de provisionamento:

> 1. A provis√£o do reposit√≥rio da infra-base; [`infra-base`](https://github.com/ns-fiap-tc/infra-base);
> 2. A provis√£o do reposit√≥rio do banco de dados: [`infra-bd`](https://github.com/ns-fiap-tc/infra-bd);
> 3. A provis√£o deste reposit√≥rio [`tech_challenge_fiap`]([#como-rodar-o-projeto](https://github.com/ns-fiap-tc/tech_challenge_fiap?tab=readme-ov-file#como-rodar-o-projeto));
> 4. A provis√£o da lambda e api gateway: [`lambda`](https://github.com/ns-fiap-tc/lambda)

> ‚ö†Ô∏è Todos os workflows s√£o configurados para serem disparados com seguran√ßa usando vari√°veis armazenadas via GitHub Secrets.

## üöÄ Como rodar o projeto

### ü§ñ Via Github Actions
<details>
  <summary>Passo a passo</summary>

#### üìñ Resumo
Ap√≥s o build e publica√ß√£o das imagens Docker da aplica√ß√£o (realizado na pipeline `Build and Push Docker Images`), uma **segunda pipeline √© acionada automaticamente** com o objetivo de **provisionar a infraestrutura na AWS utilizando Terraform**.
Este processo √© orquestrado pelo workflow `Terraform Deploy`.
> Neste caso, somente os membros da equipe que fazem parte do projeto podem utilizar este fluxo.

#### üîê Pr√©-requisitos
Antes de utilizar esse fluxo, √© necess√°rio que as seguintes **secrets** estejam configuradas no reposit√≥rio no GitHub:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` *(se estiver usando AWS Academy)*
- `TF_VAR_db_username`
- `TF_VAR_db_password`
- `DOCKERHUB_USERNAME`
- `DOCKERHUB_ACCESS_TOKEN`

Essas vari√°veis s√£o utilizadas pelo Terraform para acessar a AWS, provisionar a infraestrutura e autenticar no Docker Hub para baixar as imagens da aplica√ß√£o.

> Voc√™ pode configurar essas secrets em: `Settings > Secrets and variables > Actions`

#### ‚öôÔ∏è Etapas do Deploy via GitHub Actions:
1. ‚úÖ **Disparo autom√°tico**: A action √© iniciada **somente ap√≥s a finaliza√ß√£o com sucesso** da pipeline de build (`workflow_run.conclusion == 'success'`).
2. üßæ **Checkout do c√≥digo**: A action clona o reposit√≥rio na VM tempor√°ria usada pela GitHub Action.
3. ‚öíÔ∏è **Configura√ß√£o do Terraform**: A ferramenta `terraform` √© instalada no ambiente.
4. üìÅ **Acesso √† pasta `terraform/`**: Todas as a√ß√µes ocorrem dentro dessa pasta, que cont√©m os arquivos `.tf`.
5. üîê **Carregamento de vari√°veis sens√≠veis**:
   - Credenciais da AWS (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN`)
   - Credenciais de banco (`TF_VAR_db_username`, `TF_VAR_db_password`)
   - Credenciais do Docker Hub
6. üß™ **Execu√ß√£o do `terraform init`**: Inicializa os plugins e configura√ß√µes da infraestrutura.
7. üîç **Execu√ß√£o do `terraform plan`**: Exibe no log o que ser√° criado/modificado/destru√≠do na AWS.
8. üöÄ **Execu√ß√£o do `terraform apply`**: Provisiona automaticamente a infraestrutura, sem necessidade de confirma√ß√£o (`-auto-approve`).

#### üß≠ Diagrama do Fluxo de Execu√ß√£o
```mermaid
flowchart TD
    subgraph Build_Pipeline
        A[Build and Push Docker Images]
    end

    A -->|on success| B[Terraform Deploy]

    subgraph Terraform_Deploy
        B1[Checkout do c√≥digo]
        B2[Setup Terraform]
        B3[Carrega Secrets da AWS e do DockerHub]
        B4[terraform init]
        B5[terraform plan]
        B6[terraform apply]
    end

    B --> B1 --> B2 --> B3 --> B4 --> B5 --> B6 --> AWS[AWS Infra Provisionada]
```

#### Benef√≠cios desse fluxo
- üí° Automa√ß√£o completa: nenhuma interven√ß√£o manual √© necess√°ria ap√≥s o push.
- üîê Seguran√ßa: uso de GitHub Secrets para vari√°veis sens√≠veis.
- üîÅ Reprodutibilidade: o mesmo ambiente pode ser criado quantas vezes for necess√°rio.
- üì¶ Infra como c√≥digo (IaC): toda a infraestrutura √© descrita em arquivos .tf, versionados no reposit√≥rio.
</details>

### üíª Localmente

<details>
  <summary>Passo a passo</summary>

#### Pr√©-requisitos

Antes de come√ßar, certifique-se de ter os seguintes itens instalados e configurados em seu ambiente:

1. **Terraform**: A ferramenta que permite definir, visualizar e implantar a infraestrutura de nuvem.
2. **AWS CLI**: A interface de linha de comando da AWS.
3. **Credenciais AWS v√°lidas**: Voc√™ precisar√° de uma chave de acesso e uma chave secreta para autenticar com a AWS (no momento, o reposit√≥rio usa chaves e credenciais fornecidas pelo [AWS Academy](https://awsacademy.instructure.com/) e que divergem de contas padr√£o). Tais credenciais devem ser inseridas no arquivo `credentials` que fica dentro da pasta `.aws`

## Como usar

1. **Clone este reposit√≥rio**:

```bash
git clone https://github.com/ns-fiap-tc/tech_challenge_fiap
```

2. **Acesse o diret√≥rio do reposit√≥rio**:

```bash
cd tech_challenge_fiap
```

3. **Defina as vari√°veis necess√°rias ao n√≠vel de ambiente, criando um arquivo `.env` de acordo com o arquivo `.env.exemplo`. Exemplo:**:

```bash
DOCKERHUB_USERNAME="dockerhub_username"
DOCKERHUB_ACCESS_TOKEN="dokerhub_token"
```

4. **Inicialize o diret√≥rio Terraform**:

```bash
terraform init
```

5. **Visualize as mudan√ßas que ser√£o feitas**:

```bash
./terraform.sh plan
```

6. **Provisione a infraestrutura**:

```bash
./terraform.sh apply -auto-approve
```

7. **Para destruir a infraestrutura provisionada**:

```bash
./terraform.sh destroy -auto-approve
```

</details>

## üß± Sobre o Terraform
Este e todos os demais reposit√≥rios do projeto usam Terraform para provisionar e gerenciar a infraestrutura da aplica√ß√£o na AWS

### üß† Utiliza√ß√£o de backend remoto (`backend.tf`)
Por padr√£o, o Terraform armazena o **state file** (arquivo `terraform.tfstate`) localmente. Esse arquivo cont√©m o "espelho" do que foi criado na infraestrutura, e √© com base nele que o Terraform sabe **o que existe**, **o que precisa ser criado**, **modificado** ou **destru√≠do**.

Em ambientes colaborativos ou com automa√ß√£o via CI/CD, usar o estado local **n√£o √© seguro** nem escal√°vel.

Sendo assim, para garantir a **consist√™ncia do estado da infraestrutura** e permitir que m√∫ltiplos usu√°rios/triggers CI/CD compartilhem o mesmo controle da stack, configuramos o Terraform para utilizar um **backend remoto** no **Amazon S3** com controle de concorr√™ncia via **DynamoDB**.

#### ü™£ 1. Amazon S3 - Armazenamento seguro do state
O arquivo `terraform.tfstate` √© armazenado dentro de um bucket no S3. Isso garante:

- üß© Que **todos os desenvolvedores e pipelines** usem o mesmo estado compartilhado
- üîê Que o arquivo esteja em um ambiente seguro, com **criptografia habilitada**
- üïí Hist√≥rico de vers√µes autom√°tico, se habilitado no bucket

Exemplo de configura√ß√£o:

```hcl
bucket = "nome-do-bucket-terraform"
key    = "tech-challenge/infra/terraform.tfstate"
```

#### üîí 2. DynamoDB - Controle de concorr√™ncia com locking
Para evitar que **duas execu√ß√µes do Terraform ocorram ao mesmo tempo** (por exemplo, dois devs ou um dev + CI), utilizamos **locking via tabela DynamoDB**.

Isso evita corrup√ß√£o no `tfstate`, garantindo que apenas **uma execu√ß√£o ocorra por vez**.

```hcl
dynamodb_table = "terraform-locks"
```

O Terraform cria um "lock" tempor√°rio enquanto o plano/aplica√ß√£o est√° em execu√ß√£o e remove ao final. Se algo falhar e o lock n√£o for removido, podemos desbloquear manualmente.

#### üìå Funcionamento resumido

```text
terraform init
‚îÇ
‚îú‚îÄ‚îÄ L√™ o arquivo backend.tf
‚îÇ
‚îú‚îÄ‚îÄ Conecta com o bucket S3 e tabela DynamoDB
‚îÇ
‚îú‚îÄ‚îÄ Verifica se j√° existe um state remoto
‚îÇ     ‚îî‚îÄ‚îÄ Se sim: sincroniza o local com o remoto
‚îÇ     ‚îî‚îÄ‚îÄ Se n√£o: cria um novo .tfstate no S3
‚îÇ
‚îú‚îÄ‚îÄ Ao executar terraform apply:
‚îÇ     ‚îú‚îÄ‚îÄ Cria lock tempor√°rio na tabela DynamoDB
‚îÇ     ‚îú‚îÄ‚îÄ Aplica as mudan√ßas
‚îÇ     ‚îî‚îÄ‚îÄ Atualiza o tfstate no bucket S3
‚îÇ     ‚îî‚îÄ‚îÄ Libera o lock no DynamoDB
```
> Para observar isso na pr√°tica, perceba que ao executar `terraform init`, nos logs vai constar a conex√£o com o backend remoto.

#### ‚úÖ Benef√≠cios dessa abordagem
- üë• **Trabalho em equipe sem conflitos**
- üîÅ **Execu√ß√£o segura via CI/CD**
- ‚òÅÔ∏è **State persistente e acess√≠vel de qualquer lugar**
- üõ°Ô∏è **Prote√ß√£o contra concorr√™ncia com lock autom√°tico**

</details>


### 4Ô∏è‚É£ Fase 4

Microservicos existentes nesta fase do projeto:

- Categoria - utiliza banco de dados PostgreSQL.
- Pagamento - utiliza banco de dados MongoDB.
- Pagamento-Mock (j√° existente previamente) - n√£o utiliza banco de dados, simula o processo do Mercado Pago.
- Produto - utiliza banco de dados PostgreSQL.

Os testes unit√°rios s√£o realizados end-to-end, com o banco de dados sendo executado em container a partir do framework Testcontainer a partir dos pr√≥prios testes.  Para a correta execu√ß√£o, o ambiente que executar√° os testes deve estar com o servi√ßo do docker em execu√ß√£o.

Nesta abordagem foram utilizados mocks apenas nos clientes dos servi√ßos externos, e mesmo assim foi verificada a execu√ß√£o dos m√©todos dos mocks.

Para uma melhor utiliza√ß√£o e manuten√ß√£o de componentes compartilhados por mais de um microservi√ßo, foram criados alguns pacotes com classes auxiliares, s√£o eles:

- categoria-commons
- pagamento-commons
- pedido-commons
- produto-commons

#### Cobertura de Testes
Abaixo evid√™ncia de cobertura dos testes + link de refer√™ncia para os dados do Sonar
<img width="1755" height="216" alt="Screenshot 2025-07-29 at 12 08 33" src="https://github.com/user-attachments/assets/98b418de-e795-44a1-8d89-f26bb9407fea" />
<img width="1417" height="946" alt="Screenshot 2025-07-29 at 12 09 13" src="https://github.com/user-attachments/assets/21a537ae-9567-4b4f-81cc-d85ba3744981" />

- Link Sonar: https://sonarcloud.io/summary/new_code?id=ns-fiap-tc_tech_challenge_fiap&branch=main

<details>
  <summary>Detalhamento execu√ß√£o na Fase 4</summary>

## üëü Passos para o provisionamento
Este projeto faz parte de um ecossistema maior, composto por m√∫ltiplos reposit√≥rios que se comunicam entre si e tamb√©m utilizam GitHub Actions para provisionamento ou deploy automatizado.

> Para completo funcionamento da plataforma, √© necess√°rio seguir o seguinte fluxo de provisionamento:
> 1. A provis√£o deste reposit√≥rio; [infra-base](https://github.com/ns-fiap-tc/infra-base)
> 2. A provis√£o do reposit√≥rio dos bancos de dados: [infra-bd](https://github.com/ns-fiap-tc/infra-bd);
> 3. A provis√£o do reposit√≥rio do microsservi√ßo de categoria: [tech_challenge_fiap_ms_categoria](https://github.com/ns-fiap-tc/tech_challenge_fiap_ms_categoria);
> 4. A provis√£o do reposit√≥rio do microsservi√ßo de produto: [tech_challenge_fiap_ms_produto](https://github.com/ns-fiap-tc/tech_challenge_fiap_ms_produto);
> 5. A provis√£o do reposit√≥rio do microsservi√ßo de pagamento e pagamento-mock: [tech_challenge_fiap_ms_pagamento](https://github.com/ns-fiap-tc/tech_challenge_fiap_ms_pagamento);
> 6. A provis√£o da aplica√ß√£o principal: [tech_challenge_fiap](#como-rodar-o-projeto).


> ‚ö†Ô∏è Todos os workflows s√£o configurados para serem disparados com seguran√ßa usando vari√°veis armazenadas via GitHub Secrets.

## üöÄ Como rodar o projeto

### ü§ñ Via Github Actions
<details>
  <summary>Passo a passo</summary>

#### üìñ Resumo
Ap√≥s o build e publica√ß√£o das imagens Docker da aplica√ß√£o (realizado na pipeline `Build and Push Docker Images`), uma **segunda pipeline √© acionada automaticamente** com o objetivo de **provisionar a infraestrutura na AWS utilizando Terraform**.
Este processo √© orquestrado pelo workflow `Terraform Deploy`.
> Neste caso, somente os membros da equipe que fazem parte do projeto podem utilizar este fluxo.

#### üîê Pr√©-requisitos
Antes de utilizar esse fluxo, √© necess√°rio que as seguintes **secrets** estejam configuradas no reposit√≥rio no GitHub:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` *(se estiver usando AWS Academy)*
- `TF_VAR_db_username`
- `TF_VAR_db_password`
- `DOCKERHUB_USERNAME`
- `DOCKERHUB_ACCESS_TOKEN`

Essas vari√°veis s√£o utilizadas pelo Terraform para acessar a AWS, provisionar a infraestrutura e autenticar no Docker Hub para baixar as imagens da aplica√ß√£o.

> Voc√™ pode configurar essas secrets em: `Settings > Secrets and variables > Actions`

#### ‚öôÔ∏è Etapas do Deploy via GitHub Actions:
1. ‚úÖ **Disparo autom√°tico**: A action √© iniciada **somente ap√≥s a finaliza√ß√£o com sucesso** da pipeline de build (`workflow_run.conclusion == 'success'`).
2. üßæ **Checkout do c√≥digo**: A action clona o reposit√≥rio na VM tempor√°ria usada pela GitHub Action.
3. ‚öíÔ∏è **Configura√ß√£o do Terraform**: A ferramenta `terraform` √© instalada no ambiente.
4. üìÅ **Acesso √† pasta `terraform/`**: Todas as a√ß√µes ocorrem dentro dessa pasta, que cont√©m os arquivos `.tf`.
5. üîê **Carregamento de vari√°veis sens√≠veis**:
   - Credenciais da AWS (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN`)
   - Credenciais de banco (`TF_VAR_db_username`, `TF_VAR_db_password`)
   - Credenciais do Docker Hub
6. üß™ **Execu√ß√£o do `terraform init`**: Inicializa os plugins e configura√ß√µes da infraestrutura.
7. üîç **Execu√ß√£o do `terraform plan`**: Exibe no log o que ser√° criado/modificado/destru√≠do na AWS.
8. üöÄ **Execu√ß√£o do `terraform apply`**: Provisiona automaticamente a infraestrutura, sem necessidade de confirma√ß√£o (`-auto-approve`).

#### üß≠ Diagrama do Fluxo de Execu√ß√£o
```mermaid
flowchart TD
    subgraph Build_Pipeline
        A[Build and Push Docker Images]
    end

    A -->|on success| B[Terraform Deploy]

    subgraph Terraform_Deploy
        B1[Checkout do c√≥digo]
        B2[Setup Terraform]
        B3[Carrega Secrets da AWS e do DockerHub]
        B4[terraform init]
        B5[terraform plan]
        B6[terraform apply]
    end

    B --> B1 --> B2 --> B3 --> B4 --> B5 --> B6 --> AWS[AWS Infra Provisionada]
```

#### Benef√≠cios desse fluxo
- üí° Automa√ß√£o completa: nenhuma interven√ß√£o manual √© necess√°ria ap√≥s o push.
- üîê Seguran√ßa: uso de GitHub Secrets para vari√°veis sens√≠veis.
- üîÅ Reprodutibilidade: o mesmo ambiente pode ser criado quantas vezes for necess√°rio.
- üì¶ Infra como c√≥digo (IaC): toda a infraestrutura √© descrita em arquivos .tf, versionados no reposit√≥rio.
</details>

### üíª Localmente

<details>
  <summary>Passo a passo</summary>

#### Pr√©-requisitos

Antes de come√ßar, certifique-se de ter os seguintes itens instalados e configurados em seu ambiente:

1. **Terraform**: A ferramenta que permite definir, visualizar e implantar a infraestrutura de nuvem.
2. **AWS CLI**: A interface de linha de comando da AWS.
3. **Credenciais AWS v√°lidas**: Voc√™ precisar√° de uma chave de acesso e uma chave secreta para autenticar com a AWS (no momento, o reposit√≥rio usa chaves e credenciais fornecidas pelo [AWS Academy](https://awsacademy.instructure.com/) e que divergem de contas padr√£o). Tais credenciais devem ser inseridas no arquivo `credentials` que fica dentro da pasta `.aws`

## Como usar

1. **Clone este reposit√≥rio**:

```bash
git clone https://github.com/ns-fiap-tc/tech_challenge_fiap
```

2. **Acesse o diret√≥rio do reposit√≥rio**:

```bash
cd tech_challenge_fiap
```

3. **Defina as vari√°veis necess√°rias ao n√≠vel de ambiente, criando um arquivo `.env` de acordo com o arquivo `.env.exemplo`. Exemplo:**:

```bash
DOCKERHUB_USERNAME="dockerhub_username"
DOCKERHUB_ACCESS_TOKEN="dokerhub_token"
```

4. **Inicialize o diret√≥rio Terraform**:

```bash
terraform init
```

5. **Visualize as mudan√ßas que ser√£o feitas**:

```bash
./terraform.sh plan
```

6. **Provisione a infraestrutura**:

```bash
./terraform.sh apply -auto-approve
```

7. **Para destruir a infraestrutura provisionada**:

```bash
./terraform.sh destroy -auto-approve
```

</details>

## üß± Sobre o Terraform
Este e todos os demais reposit√≥rios do projeto usam Terraform para provisionar e gerenciar a infraestrutura da aplica√ß√£o na AWS

### üß† Utiliza√ß√£o de backend remoto (`backend.tf`)
Por padr√£o, o Terraform armazena o **state file** (arquivo `terraform.tfstate`) localmente. Esse arquivo cont√©m o "espelho" do que foi criado na infraestrutura, e √© com base nele que o Terraform sabe **o que existe**, **o que precisa ser criado**, **modificado** ou **destru√≠do**.

Em ambientes colaborativos ou com automa√ß√£o via CI/CD, usar o estado local **n√£o √© seguro** nem escal√°vel.

Sendo assim, para garantir a **consist√™ncia do estado da infraestrutura** e permitir que m√∫ltiplos usu√°rios/triggers CI/CD compartilhem o mesmo controle da stack, configuramos o Terraform para utilizar um **backend remoto** no **Amazon S3** com controle de concorr√™ncia via **DynamoDB**.

#### ü™£ 1. Amazon S3 - Armazenamento seguro do state
O arquivo `terraform.tfstate` √© armazenado dentro de um bucket no S3. Isso garante:

- üß© Que **todos os desenvolvedores e pipelines** usem o mesmo estado compartilhado
- üîê Que o arquivo esteja em um ambiente seguro, com **criptografia habilitada**
- üïí Hist√≥rico de vers√µes autom√°tico, se habilitado no bucket

Exemplo de configura√ß√£o:

```hcl
bucket = "nome-do-bucket-terraform"
key    = "tech-challenge/infra/terraform.tfstate"
```

#### üîí 2. DynamoDB - Controle de concorr√™ncia com locking
Para evitar que **duas execu√ß√µes do Terraform ocorram ao mesmo tempo** (por exemplo, dois devs ou um dev + CI), utilizamos **locking via tabela DynamoDB**.

Isso evita corrup√ß√£o no `tfstate`, garantindo que apenas **uma execu√ß√£o ocorra por vez**.

```hcl
dynamodb_table = "terraform-locks"
```

O Terraform cria um "lock" tempor√°rio enquanto o plano/aplica√ß√£o est√° em execu√ß√£o e remove ao final. Se algo falhar e o lock n√£o for removido, podemos desbloquear manualmente.

#### üìå Funcionamento resumido

```text
terraform init
‚îÇ
‚îú‚îÄ‚îÄ L√™ o arquivo backend.tf
‚îÇ
‚îú‚îÄ‚îÄ Conecta com o bucket S3 e tabela DynamoDB
‚îÇ
‚îú‚îÄ‚îÄ Verifica se j√° existe um state remoto
‚îÇ     ‚îî‚îÄ‚îÄ Se sim: sincroniza o local com o remoto
‚îÇ     ‚îî‚îÄ‚îÄ Se n√£o: cria um novo .tfstate no S3
‚îÇ
‚îú‚îÄ‚îÄ Ao executar terraform apply:
‚îÇ     ‚îú‚îÄ‚îÄ Cria lock tempor√°rio na tabela DynamoDB
‚îÇ     ‚îú‚îÄ‚îÄ Aplica as mudan√ßas
‚îÇ     ‚îî‚îÄ‚îÄ Atualiza o tfstate no bucket S3
‚îÇ     ‚îî‚îÄ‚îÄ Libera o lock no DynamoDB
```
> Para observar isso na pr√°tica, perceba que ao executar `terraform init`, nos logs vai constar a conex√£o com o backend remoto.

#### ‚úÖ Benef√≠cios dessa abordagem
- üë• **Trabalho em equipe sem conflitos**
- üîÅ **Execu√ß√£o segura via CI/CD**
- ‚òÅÔ∏è **State persistente e acess√≠vel de qualquer lugar**
- üõ°Ô∏è **Prote√ß√£o contra concorr√™ncia com lock autom√°tico**

</details>


## ‚ú® Contribuidores

- Guilherme Fausto - RM 359909
- Nicolas Silva - RM 360621
- Rodrigo Medda Pereira - RM 360575

## Licen√ßa

[![Licence](https://img.shields.io/github/license/Ileriayo/markdown-badges?style=for-the-badge)](./LICENSE)
