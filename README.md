# FIAP Tech Challenge

![Spring](https://img.shields.io/badge/spring-%236DB33F.svg?style=for-the-badge&logo=spring&logoColor=white)
![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
![Maven](https://img.shields.io/badge/maven-%230db7ed.svg?style=for-the-badge&logo=maven&logoColor=white)
![Swagger](https://img.shields.io/badge/-Swagger-%23Clojure?style=for-the-badge&logo=swagger&logoColor=white)
![RabbitMQ](https://img.shields.io/badge/Rabbitmq-FF6600?style=for-the-badge&logo=rabbitmq&logoColor=white)

[![CI Status](https://github.com/ra1nmak3r1/tech_challenge_fiap/actions/workflows/docker-build.yml/badge.svg)](https://github.com/ra1nmak3r1/tech_challenge_fiap/actions)

## üìñ Sobre o projeto 

Tech Challenge do curso Software Architecture da FIAP. 

### 1Ô∏è‚É£ Fase 1
> Aplica√ß√£o desenvolvida utilizando arquitetura hexagonal que contempla a gest√£o dos pedidos de uma lanchonete.
>
> O c√≥digo fonte inalterado desta fase ainda pode ser encontrado na branch [`release/v1.0.0`](https://github.com/ra1nmak3r1/tech_challenge_fiap/tree/release/v1.0.0)

### 2Ô∏è‚É£ Fase 2
> Migra√ß√£o da aplica√ß√£o da arquitetura hexagonal para clean architecture.

### üìù Sobre a refatora√ß√£o na aplicac√£o para a Fase 2

1. Por conta do refactoring para clean architecture, uma situa√ß√£o que enfrentamos foi a aus√™ncia do contexto transacional do Spring na utiliza√ß√£o das classes de neg√≥cios quando executavam o m√≥dulo de persist√™ncia (JPA), uma vez que as classes de neg√≥cios (*UseCasesImpl) n√£o estavam mais sendo gerenciadas pelo ApplicationContext do Spring. Como solu√ß√£o para este cen√°rio, utilizamos AOP (Programa√ß√£o Orientada a Aspectos) para interceptar as chamadas aos m√©todos dos Controllers (que est√£o sendo gerenciados pelo Spring) para incluirmos cada execu√ß√£o em uma transa√ß√£o isolada.

2. Alteramos a estrutura do projeto em sub-m√≥dulos, sendo eles:
	- business: cont√©m as classes de neg√≥cios, que fazem parte do core da aplica√ß√£o e que podem ser executados com diferentes recursos externos, sendo utilizados os frameworks: lombok e mapstruct - ambos utilizados na gera√ß√£o de c√≥digo em tempo de compila√ß√£o.
	- app: cont√©m as classes relacionadas aos frameworks e recursos utilizados para o correto funcionamento da aplica√ß√£o.
	- pagamento-mock: aplica√ß√£o apartada que simula a execu√ß√£o do Mercado Pago para efetiva√ß√£o do pagamento do pedido. 

3. No m√≥dulo business, foram utilizados apenas os frameworks lombok e mapstruct.

    - O lombok √© utilizado para a gera√ß√£o de m√©todos getters, setters, hashCode, equals e construtores.

    - Enquanto o mapstruct √© utilizado para a cria√ß√£o de m√©todos que fazem o mapeamento dos atributos entre entidades para realiza√ß√£o da c√≥pia dos valores dos atributos de beans de classes diferentes.

4. Utilizamos os presenters apenas como sendo a transforma√ß√£o dos beans de dom√≠nio pra os DTOs a serem enviados para fora dos Controllers.  Nesta implementa√ß√£o os DTOs s√£o os mesmos utilizados no recebimento dos m√©todos externos e como informa√ß√£o a ser retornada, mas em caso de altera√ß√£o da informa√ß√£o retornada, basta alterar o tipo de retorno dos m√©todos dos Controllers e os presenters. 

### 2Ô∏è‚É£ Fase 3
> Migra√ß√£o da aplica√ß√£o para AWS, automatizando a cria√ß√£o da infra-estrutura com o terraform.
1. O banco de dados foi migrado para a AWS RDS utilizando o engine do PostgreSQL, que era o banco que j√° era usado pela aplica√ß√£o. N√£o foram realizadas altera√ß√µes na estrutura da base de dados, por j√° existirem todos os campos necess√°rios.

2. Foi inclu√≠da a utiliza√ß√£o do servi√ßo AWS Lambda para consultar a exist√™ncia do CPF do cliente na base de dados, caso seja informado inicialmente. Sendo inclu√≠do o CPF no JWT que ser√° criado neste momento.  Caso n√£o exista ou n√£o seja informado, o JWT ser√° criado com CPF vazio. 

## üèõÔ∏è Estrutura utilizada nos pacotes


```
ra√≠z
‚îú‚îÄ‚îÄ app (m√≥dulo)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src
‚îÇ       ‚îî‚îÄ‚îÄ application
‚îÇ           ‚îú‚îÄ‚îÄ	device
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ queue (produtores / consumidores)
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ rest (interfaces)
‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exception
‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handler
‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ impl (implementa√ß√µes das interfaces)
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ persistence
‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ entity
‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ mapper
‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ repository
‚îÇ           ‚îî‚îÄ‚îÄ infrastructure (local onde ser√£o utilizadas as depend√™ncias de cada cloud ou de recursos externos)
‚îÇ               ‚îú‚îÄ‚îÄ aspect (pacote contendo as classes da AOP)
‚îÇ               ‚îú‚îÄ‚îÄ config (inclus√£o das configura√ß√µes da aplica√ß√£o, como por exemplo @Configuration do Spring, criando os @Bean)
‚îÇ               ‚îú‚îÄ‚îÄ utils (classes utilit√°rias)
‚îÇ               ‚îî‚îÄ‚îÄ aws (pacotes espec√≠ficos para cada cloud, por exemplo)
‚îú‚îÄ‚îÄ business (m√≥dulo)
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src
‚îÇ       ‚îî‚îÄ‚îÄ business (classes / interfaces referentes √†s regras de neg√≥cios da aplica√ß√£o. criar as classes / interfaces sem usar frameworks - c√≥digo o mais simples poss√≠vel)
‚îÇ           ‚îú‚îÄ‚îÄ adapter
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ controller
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ gateway
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ presenter
‚îÇ           ‚îú‚îÄ‚îÄ common
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ dto
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ mapper
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ queue (produtor / consumidor)
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ persistence
‚îÇ           ‚îî‚îÄ‚îÄ core
‚îÇ               ‚îú‚îÄ‚îÄ domain (POJOs)
‚îÇ               ‚îú‚îÄ‚îÄ exception
‚îÇ               ‚îî‚îÄ‚îÄ usecase (interfaces contendo os m√©todos a serem implementados)
‚îÇ                   ‚îî‚îÄ‚îÄ impl (implementa√ß√µes das interfaces)
‚îî‚îÄ‚îÄ pagamento-mock (m√≥dulo)
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ pom.xml
    ‚îî‚îÄ‚îÄ src
        ‚îî‚îÄ‚îÄ pagamentomock
            ‚îú‚îÄ‚îÄ adapter
            ‚îÇ   ‚îú‚îÄ‚îÄ input
            ‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ controller
            ‚îÇ	‚îÇ    ‚îú‚îÄ‚îÄ dto
            ‚îÇ	‚îÇ    ‚îî‚îÄ‚îÄ mapper
            ‚îÇ	‚îî‚îÄ‚îÄ output
            ‚îî‚îÄ‚îÄ infrastructure
                ‚îú‚îÄ‚îÄ config
                ‚îî‚îÄ‚îÄ utils

```

## üíª Tecnologias utilizadas na Aplica√ß√£o

* Maven 3.9.9
* Spring Boot 3.3.4
* Java 17

## üì¶ Arquitetura da Infraestrutura e CI/CD

### üöÄ Tecnologias Utilizadas na Infraestrutura

- **Minikube** ‚Äî Cluster Kubernetes local para simular produ√ß√£o
- **Kubernetes (K8s)** ‚Äî Orquestra√ß√£o dos recursos
- **Docker** ‚Äî Empacotamento das aplica√ß√µes em containers
- **Docker Compose** ‚Äî Suporte ao ambiente de desenvolvimento local
- **GitHub Actions** ‚Äî Pipeline de CI para build e deploy das imagens
- **Docker Hub** ‚Äî Reposit√≥rio para armazenar imagens da aplica√ß√£o
- **Secrets e ConfigMaps** ‚Äî Gest√£o segura de vari√°veis sens√≠veis no cluster
- **RabbitMQ(4.0.5) & PostgreSQL(16)** ‚Äî Infraestrutura de mensageria e banco de dados

---

### ‚úÖ Pr√©-requisitos para Execu√ß√£o

- **Docker** e **Docker Compose** instalados
- **Minikube** instalado e configurado localmente (Testes e valida√ß√µes realizados com a v1.35.0)
- Acesso ao `.env` com as vari√°veis necess√°rias

---

### üõ†Ô∏è Integra√ß√£o Cont√≠nua (CI)

O fluxo de CI √© automatizado via **GitHub Actions** e √© engatilhado a cada `push` na branch `main`.

1. Faz o checkout do reposit√≥rio
2. Gera as imagens Docker de cada aplica√ß√£o (`app` e `mock-pagamento`)
3. Faz o login no Docker Hub usando um **Access Token seguro**
4. Publica as imagens no Docker Hub (`app` em reposit√≥rio privado e `mock-pagamento` em reposit√≥rio p√∫blico)

---

### üåê Deploy e Infraestrutura (CD)

A subida do ambiente √© feita localmente via script `setup.sh` ou `setup.bat`, que:

1. L√™ e carrega o arquivo `.env` com credenciais e configura√ß√µes
2. Cria dinamicamente as Secrets no Kubernetes
3. Aplica todos os manifestos do cluster (PostgreSQL, RabbitMQ, app e mock)
4. Exp√µe os servi√ßos via `port-forward` para acesso local (`localhost:8080`, `:8081`)

---

### üß≠ Fluxo da Arquitetura (CI/CD)

```mermaid
flowchart TD
    dev[Desenvolvedor] --> pushGit[Push para GitHub]

    subgraph GitHub_Actions_CI
        pushGit --> buildApp[Build imagem da aplica√ß√£o]
        buildApp --> buildMock[Build imagem do mock-pagamento]
        buildMock --> dockerLogin[Login no Docker Hub]
        dockerLogin --> pushImages[Push das imagens]
    end

    pushImages --> dockerHub[Docker Hub]

    dev --> runSetup[Executa setup.sh ou setup.bat]

    subgraph Execucao_Local_CD
        runSetup --> loadEnv[Carrega vari√°veis do .env]
        loadEnv --> createSecrets[Cria Secrets no Kubernetes]
        createSecrets --> applyManifests[Aplica manifestos do cluster]
        applyManifests --> portForward[Port-forward dos servi√ßos]
    end

    dockerHub --> applyManifests
    portForward --> appAccess[Acesso ao App Principal na porta 8080]
    portForward --> mockAccess[Acesso ao Mock Pagamento na porta 8081]
```

### üìà Escalabilidade e HPA no Kubernetes

Para lidar com cen√°rios de alta demanda, como por exemplo **lentid√£o no totem da lanchonete durante hor√°rios de pico**, a aplica√ß√£o principal foi configurada com um recurso chamado **HPA (Horizontal Pod Autoscaler)** no Kubernetes.

O HPA monitora o uso de **CPU** do container da aplica√ß√£o e **escala automaticamente o n√∫mero de r√©plicas** (pods) quando a utiliza√ß√£o ultrapassa um determinado limite configurado.

---

#### üìå Exemplo de caso pr√°tico

> Cen√°rio: durante o hor√°rio de almo√ßo, h√° um grande volume de clientes utilizando o totem de autoatendimento. Isso gera lentid√£o e demora nas respostas da aplica√ß√£o.

üîß Solu√ß√£o:

- O HPA detecta que o uso de CPU no pod principal (`app`) ou no `mock-pagamento` est√° acima do limite (70% para o app, 80% para o mock)
- Ele automaticamente cria novos pods (`r√©plicas`) da aplica√ß√£o para distribuir a carga
- Os **Services do Kubernetes** atuam como balanceadores de carga, redirecionando requisi√ß√µes para os pods dispon√≠veis
- Quando o pico passa, o HPA reduz o n√∫mero de pods novamente para economizar recursos

---

#### üß≠ Diagrama da Escalabilidade

```mermaid
flowchart TD
    Client[Usuario - Totem/Navegador] --> Ingress[Entrada de Requisi√ß√µes]
    Ingress --> AppService[Service - app]
    Ingress --> MockService[Service - mock-pagamento]

    subgraph App_Pods
        App1[Pod app - R√©plica 1]
        App2[Pod app - R√©plica 2]
        AppN[Pod app - R√©plica N]
    end

    subgraph Mock_Pods
        Mock1[Pod mock - R√©plica 1]
        Mock2[Pod mock - R√©plica 2]
        MockN[Pod mock - R√©plica N]
    end

    AppService --> App1
    AppService --> App2
    AppService --> AppN

    MockService --> Mock1
    MockService --> Mock2
    MockService --> MockN

    subgraph HPA[Horizontal Pod Autoscaler]
        HPAApp[Escala o app baseado em CPU >70%]
        HPAMock[Escala o mock baseado em CPU >80%]
    end

    HPAApp --> App1
    HPAApp --> App2
    HPAMock --> Mock1
    HPAMock --> Mock2
```

---

#### üßë‚Äçüíª Considera√ß√µes

- O **HPA est√° configurado para ambos os servi√ßos**:
  - `app` com threshold de 70% de uso de CPU
  - `mock-pagamento` com threshold de 80% de uso de CPU
- O `app` pode escalar at√© **5 r√©plicas**, conforme demanda
- O `mock-pagamento` pode escalar at√© **3 r√©plicas**, conforme demanda
> No `mock-pagamento` estamos apenas simulando um sistema externo de pagamentos, n√£o necessariamente precisar√≠amos de um HPA nele, mas decidimos manter a configura√ß√£o em uma escala menor

## ‚öôÔ∏è Como executar a infraestrutura com Minikube

### ‚úÖ 1. Pr√©-requisitos

Instale as ferramentas abaixo:

- [Docker](https://www.docker.com/products/docker-desktop/)
- [Minikube](https://minikube.sigs.k8s.io/docs/start/)

---

### ‚úÖ 2. Clonar o reposit√≥rio

```bash
git clone https://github.com/ra1nmak3r1/tech_challenge_fiap.git
cd tech_challenge_fiap
```

---

### ‚úÖ 3. Criar o arquivo `.env` com base no `.env.example`

J√° existe um arquivo de exemplo chamado **`.env.example`** no projeto.

<details>
  <summary><strong>üîê COMO CONFIGURAR O ARQUIVO .ENV</strong></summary>

1. Copie o arquivo `.env.example` para `.env`:

```bash
cp .env.example .env  # Linux ou Mac
```

```powershell
copy .env.example .env  # Windows
```

2. Substitua os valores fict√≠cios pelos **valores reais que foram enviados separadamente**.

> ‚ö†Ô∏è Os valores do `.env.example` s√£o apenas ilustrativos e n√£o funcionais.
</details>

---

### ‚úÖ 4. Subir a infraestrutura
Foram desenvolvidos scripts em `.sh` e `.bat` para facilitar a inicializa√ß√£o da infraestrutura no Minikube, sem que seja necess√°rio executar os comandos da API do Kubernetes para tal. Na pr√°tica ambos os scripts fazem o seguinte:

1. ‚úÖ Verifica se o arquivo `.env` existe e carrega suas vari√°veis
2. üîê Cria dinamicamente as **Secrets** no Kubernetes com base no `.env`
3. üê≥ Cria uma Secret para autentica√ß√£o no **Docker Hub** (para acesso √† imagem privada da aplica√ß√£o principal)
4. üöÄ Inicializa o cluster local do **Minikube**
5. üìÇ Aplica todos os **manifestos Kubernetes** da aplica√ß√£o:
   - Banco de dados PostgreSQL
   - RabbitMQ
   - Aplica√ß√£o principal (`app`)
   - Servi√ßo de mock de pagamento (`mock-pagamento`)
6. üåê Exp√µe os servi√ßos localmente via `kubectl port-forward`, permitindo acesso via `localhost`
7. ‚è≥ Aguarda os pods ficarem prontos antes de liberar o acesso

> ‚ÑπÔ∏è Em cerca de 3-4 minutos, o ambiente estar√° funcionando localmente com todos os microsservi√ßos no ar.

#### ‚ñ∂Ô∏è Linux ou Mac:

```bash
chmod +x setup.sh
./setup.sh
```

#### ü™ü Windows:

```powershell
.\setup.bat
```

---

### ‚úÖ 5. Acessar os servi√ßos localmente

| Servi√ßo        | URL                         |
|----------------|-----------------------------|
| Aplica√ß√£o      | http://localhost:8080       |
| Mock Pagamento | http://localhost:8081       |

---

### üõ†Ô∏è Comandos √∫teis para observa√ß√£o

Ver todos os pods:
```bash
kubectl get pods
```

Ver logs da aplica√ß√£o principal:
```bash
kubectl logs -l app=lanchonete-app -f
```

Ver logs do mock pagamento:
```bash
kubectl logs -l app=mock-pagamento -f
```

---

### üßπ Resetar tudo (opcional)

Caso queira limpar o ambiente e recome√ßar do zero, preparamos os seguintes execut√°veis para facilitar o processo:

#### ‚ñ∂Ô∏è Linux ou Mac
```bash
./delete_setup.sh
```
#### ü™ü Windows
```powershell
.\delete_setup.bat
```
Ou, voc√™ tamb√©m pode zerar o minikube por completo se desejar
```bash
minikube delete
```

## üìÑ Acesso √† documenta√ß√£o das APIs

#### Aplica√ß√£o
* http://localhost:8080/api-docs (endpoints)
* http://localhost:8080/swagger-ui/index.html (swagger-ui)

#### Pagamento Mock
* http://localhost:8081/api-docs (endpoints)
* http://localhost:8081/swagger-ui/index.html (swagger-ui)


## üß™ Execu√ß√£o em modo de Desenvolvimento (sem Minikube)
<details>

<summary>Se desejar executar a aplica√ß√£o em modo de desenvolvimento local para debugar e alterar o c√≥digo fonte em car√°ter de teste, siga este passo a passo</summary>

#### ‚úÖ 1. Pr√©-requisitos

* Docker
* Docker Compose
* Maven 3.9.9
* Spring Boot 3.3.4
* Java 17

---

#### ‚úÖ 2. Gerar o `.env`

Crie o arquivo `.env` com base no `.env.example`, da mesma forma descrita  anteriormente:

```bash
cp .env.example .env  # Linux ou Mac
```

```powershell
copy .env.example .env  # Windows
```

Substitua os valores conforme os dados enviados.

---

#### ‚úÖ 3. Buildar as aplica√ß√µes localmente (apenas na primeira vez)

Primeiro instale o pacote parent da aplica√ß√£o, atrav√©s do comando:

```bash
mvn -DskipTests -DskipITs=true -N clean install 
```

Em seguida, compile o projeto e gere o arquivo `.jar`. Para isso, execute:

```bash
mvn -DskipTests clean package
```

---

#### ‚úÖ 4. Subir o ambiente de desenvolvimento com Docker Compose

Na raiz do projeto, execute:

```bash
docker compose up --build
```

Isso ir√°:

- Buildar os containers da aplica√ß√£o principal e do mock
- Subir o banco de dados PostgreSQL e o RabbitMQ
- Conectar todos os servi√ßos em rede local

---

#### ‚úÖ 5. Acessar os servi√ßos localmente

| Servi√ßo         | URL                         |
|----------------|-----------------------------|
| Aplica√ß√£o      | http://localhost:8080       |
| Mock Pagamento | http://localhost:8081       |

</details>

## üîÑ Fluxo de Execu√ß√£o

### 1. Cria√ß√£o de um novo pedido

```
POST -> /pedido-service/v1/save
```

---

### 2. Atualiza√ß√£o dos itens do pedido

```
PUT -> /pedido-service/v1/save/:id
```

Esta atualiza√ß√£o contempla:

- Os **itens do pedido**
- As **informa√ß√µes de pagamento**
- O **status do pedido**

> O ID do objeto pagamento deve ser preenchido.

Caso o status seja alterado para `RECEBIDO`, isso significa que o pedido foi finalizado pelo usu√°rio e agora ser√° feito o **processamento do pagamento**, que ocorrer√° de forma ass√≠ncrona, utilizando o projeto **Pagamento Mock**.

---

### 3. Confirma√ß√£o do Pagamento

Para confirmar que o pagamento foi realizado, √© necess√°rio executar o endpoint abaixo do **Pagamento Mock**, que por sua vez **executar√° o webhook da aplica√ß√£o**.

**Endpoint do Pagamento Mock:**

```
POST -> /pagamento-mock-service/v1/callPagamentoWebHook/:pedidoId/:aprovarPagamento
```

**Webhook da aplica√ß√£o:**

```
POST -> /pagamento-service/v1/updateStatus/:pedidoId/:statusCode
```

### Observa√ß√µes

- Ambos os m√©todos foram definidos como `POST` por n√£o serem indepotentes.
- A execu√ß√£o do webhook, caso receba o `statusCode = 100`, significa que o pagamento foi realizado com sucesso, e far√° com que o pedido seja **confirmado** e as **Ordens de Servi√ßo sejam criadas para a cozinha**.

## ‚ú® Contribuidores
* Guilherme Fausto - RM 359909
* Nicolas Silva - RM 360621
* Rodrigo Medda Pereira - RM 360575


## Licen√ßa
[![Licence](https://img.shields.io/github/license/Ileriayo/markdown-badges?style=for-the-badge)](./LICENSE)